agent/models.py
--- a/agent/models.py
+++ b/agent/models.py
@@ -137,6 +137,29 @@ def build_model(model_cfg: Dict[str, Any], ollama_host: str, ollama_timeout_seconds:
         return OllamaModel(name, host=ollama_host, timeout_seconds=int(ollama_timeout_seconds))
 
+    return ChatModel(model_name=name)
+
+
class ChatModel:
+    """
+    Local chat model wrapper using Microsoft's Phi-2.
+
+    This is the "brain" Jarvis uses whenever no tool is chosen.
+    """
+
+    def __init__(self, model_name: str = "microsoft/phi-2"):
+        self.model_name = model_name
+        print(f"[ChatModel] Loading model '{model_name}' (this might take a moment)...")
+
+        # Load tokenizer & model
+        self.tokenizer = AutoTokenizer.from_pretrained(
+            model_name,
+            trust_remote_code=True,
+        )
+        self.model = AutoModelForCausalLM.from_pretrained(
+            model_name,
+            torch_dtype=torch.float32,      # safer default for CPU; we'll optimise later
